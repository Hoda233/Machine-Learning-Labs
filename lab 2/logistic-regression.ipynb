{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we will learn how to apply Logistic regression for predicting the cooling load requirements (Y2) of buildings as a function of building parameters (Xs).\n",
    "\n",
    "The attached dataset is taken from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency).\n",
    "\n",
    "To run this code, you will need the following python packages:\n",
    "* numpy\n",
    "* pandas\n",
    "* openpyxl\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\laptop\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\laptop\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\LAPTOP\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the dataset using pandas\n",
    "df = pd.read_excel(\"Energy_Efficiency.xlsx\", engine = 'openpyxl')\n",
    "# Remove any unnamed columns (might occur due to difference in pandas readers)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "# Remove any row with NaNs\n",
    "df = df.dropna(how='all')\n",
    "# Drop Y1 (as we only consider Y2 for classification)\n",
    "df = df.drop('Y1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we will split the dataframe into a training and testing splits with a 70% / 30% ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42) # Random is fixed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>15.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>19.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "      <td>46.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.76</td>\n",
       "      <td>661.5</td>\n",
       "      <td>416.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>33.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.86</td>\n",
       "      <td>588.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.71</td>\n",
       "      <td>710.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>30.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>29.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5  X6    X7  X8     Y2\n",
       "334  0.62  808.5  367.5  220.50  3.5   4  0.25   1  15.77\n",
       "139  0.64  784.0  343.0  220.50  3.5   5  0.10   2  19.30\n",
       "485  0.90  563.5  318.5  122.50  7.0   3  0.25   5  32.00\n",
       "547  0.79  637.0  343.0  147.00  7.0   5  0.40   1  46.94\n",
       "18   0.79  637.0  343.0  147.00  7.0   4  0.00   0  30.93\n",
       "..    ...    ...    ...     ...  ...  ..   ...  ..    ...\n",
       "71   0.76  661.5  416.5  122.50  7.0   5  0.10   1  33.67\n",
       "106  0.86  588.0  294.0  147.00  7.0   4  0.10   2  27.36\n",
       "270  0.71  710.5  269.5  220.50  3.5   4  0.10   5  14.26\n",
       "435  0.98  514.5  294.0  110.25  7.0   5  0.25   4  30.12\n",
       "102  0.90  563.5  318.5  122.50  7.0   4  0.10   2  29.36\n",
       "\n",
       "[537 rows x 9 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.760354</td>\n",
       "      <td>674.867784</td>\n",
       "      <td>318.636872</td>\n",
       "      <td>178.115456</td>\n",
       "      <td>5.201117</td>\n",
       "      <td>3.500931</td>\n",
       "      <td>0.235940</td>\n",
       "      <td>2.854749</td>\n",
       "      <td>24.287505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.104790</td>\n",
       "      <td>87.758133</td>\n",
       "      <td>43.619254</td>\n",
       "      <td>44.839207</td>\n",
       "      <td>1.750948</td>\n",
       "      <td>1.106502</td>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.544532</td>\n",
       "      <td>9.505775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>21.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>759.500000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>32.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4          X5          X6  \\\n",
       "count  537.000000  537.000000  537.000000  537.000000  537.000000  537.000000   \n",
       "mean     0.760354  674.867784  318.636872  178.115456    5.201117    3.500931   \n",
       "std      0.104790   87.758133   43.619254   44.839207    1.750948    1.106502   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.500000    2.000000   \n",
       "25%      0.660000  612.500000  294.000000  147.000000    3.500000    3.000000   \n",
       "50%      0.740000  686.000000  318.500000  220.500000    3.500000    3.000000   \n",
       "75%      0.820000  759.500000  343.000000  220.500000    7.000000    4.000000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.000000    5.000000   \n",
       "\n",
       "               X7          X8          Y2  \n",
       "count  537.000000  537.000000  537.000000  \n",
       "mean     0.235940    2.854749   24.287505  \n",
       "std      0.134118    1.544532    9.505775  \n",
       "min      0.000000    0.000000   10.940000  \n",
       "25%      0.100000    2.000000   15.500000  \n",
       "50%      0.250000    3.000000   21.160000  \n",
       "75%      0.400000    4.000000   32.920000  \n",
       "max      0.400000    5.000000   48.030000  "
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median value of the target: 21.16\n",
      "Percentage of 'high load' samples: 49.906890130353815 %\n"
     ]
    }
   ],
   "source": [
    "# Now we will extract the models input and targets from both the training and testing dataframes\n",
    "def extract_Xy(df):\n",
    "    df_numpy = df.to_numpy()\n",
    "    return df_numpy[:, :-1], df_numpy[:, -1]\n",
    "\n",
    "X_train, y_train = extract_Xy(df_train)\n",
    "X_test, y_test = extract_Xy(df_test)\n",
    "\n",
    "y_median = np.median(y_train)\n",
    "print(\"Median value of the target:\", y_median)\n",
    "\n",
    "# Since we will treat this as a classification task, we will assume that\n",
    "# the load is \"high\" (y = True) if its compressive ratio is higher than the median\n",
    "# otherwise, it is assumed to be \"low\" (y = False)\n",
    "y_train = y_train > y_median\n",
    "y_test = y_test > y_median\n",
    "\n",
    "# Now ~50% of the samples should be considered \"high\" and the rest are considered \"low\"\n",
    "print(f\"Percentage of 'high load' samples: {y_train.mean() * 100} %\")\n",
    "\n",
    "# Also, lets standardize the data since it improves the training process\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean)/(1e-8 + X_std)\n",
    "X_test = (X_test - X_mean)/(1e-8 + X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 8.97 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "model = LogisticRegression(random_state=0, penalty=\"none\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accurracy: 98.32402234636871%\n",
      "Testing Accurracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "print(f\"Training Accurracy: {accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = model.predict(X_test)\n",
    "print(f\"Testing Accurracy: {accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO: Implement sigmoid (hint: use np.exp)\n",
    "    return  1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1e2) = 3.7200759760208356e-44\n",
      "sigmoid(   0) = 0.5\n",
      "sigmoid(+1e2) = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{sigmoid(-1e2) = }\") # This should be almost equal 0\n",
    "print(f\"{sigmoid(   0) = }\") # This should be exactly 0.5\n",
    "print(f\"{sigmoid(+1e2) = }\") # This should be almost equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_accuracy_score(true, predicted):\n",
    "    #TODO: Implement an accuracy metric so that is can be used instead of Sklearn's accuracy score\n",
    "    #Note: both true and predicted will be boolean numpy array\n",
    "    acc_score = np.sum (true == predicted) / len(true)\n",
    "    return acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = 1.0\n",
      "our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = 0.5\n",
      "our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = 1.0\n",
      "our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = }\") # Should be 0.5\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = }\") # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any premade algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                            # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducability\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multipled by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "        \n",
    "        pred = sigmoid(X @ self.w)\n",
    "        error = y - pred\n",
    "        gradient = -X.T @ error\n",
    "        return gradient\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "\n",
    "        # w(t+1) = w(t) - learning rate - gradient\n",
    "        self.w = self.w - self.lr * self._gradient(X,y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "        pred = sigmoid(X @ self.w)\n",
    "        return pred > self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.3 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 10 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 100 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1000 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [1,10,100,1000] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.1, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.05, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.01, the accuracy reaches 98.76543209876543% in 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [ 0.5, 0.1, 0.05, 0.01] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nour implementation:\\n-------------------\\nperformance:\\nTraining Accuracy: 98.32402234636871%\\nTesting Accuracy: 96.53679653679653%\\n\\ntraining time:\\nCPU times: total: 15.6 ms\\nWall time: 11 ms\\n\\nScikit-Learn's implementation:\\n------------------------------\\nperformance:\\nTraining Accurracy: 98.32402234636871%\\nTesting Accurracy: 96.53679653679653%\\n\\ntraining time:\\nCPU times: total: 31.2 ms\\nWall time: 2.99 ms\\n\\n--> time can change a little bit each run \\n------------------------------------------------------\\n\\nin training time: \\nour implementation is faster than Scikit-Learn's implementation\\n\\nit may be because of small dataset and small number of epochs\\n\\nin performance:\\nBoth have almost the same performance\\nwith almost same accuracy\\n\\n\""
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Write your conclusion about your implementation's performance and training time\n",
    "\n",
    "'''\n",
    "our implementation:\n",
    "-------------------\n",
    "performance:\n",
    "Training Accuracy: 98.32402234636871%\n",
    "Testing Accuracy: 96.53679653679653%\n",
    "\n",
    "training time:\n",
    "CPU times: total: 15.6 ms\n",
    "Wall time: 8.97 ms\n",
    "\n",
    "Scikit-Learn's implementation:\n",
    "------------------------------\n",
    "performance:\n",
    "Training Accurracy: 98.32402234636871%\n",
    "Testing Accurracy: 96.53679653679653%\n",
    "\n",
    "training time:\n",
    "CPU times: total: 0 ns\n",
    "Wall time: 2.99 ms\n",
    "\n",
    "--> time can change a little bit each run \n",
    "------------------------------------------------------\n",
    "\n",
    "in training time: \n",
    "our implementation is faster than Scikit-Learn's implementation\n",
    "\n",
    "it may be because of small dataset and small number of epochs\n",
    "\n",
    "in performance:\n",
    "Both have almost the same performance\n",
    "with almost same accuracy\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "As a bonus, you can implement and test the following:\n",
    "* Stochastic gradient descent\n",
    "* Termination conditions (e.g. The gradient check)\n",
    "  \n",
    "Write your conclusion about any results you calculate for your bonus implementations.\n",
    "\n",
    "**IMPORTANT**: Do not implement the bonus in the previous cells. You can copy and paste codes from the previous cells and continue your implementation below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any premade algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                            # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducability\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multipled by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "        \n",
    "        pred = sigmoid(X @ self.w)\n",
    "        error = y - pred\n",
    "        gradient = -X.T @ error\n",
    "        return gradient\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "\n",
    "        # w(t+1) = w(t) - learning rate - gradient\n",
    "        index = np.random.randint(X.shape[0])\n",
    "        xi = X[index, :].reshape(1, -1)\n",
    "        yi = y[index]\n",
    "        self.w = self.w - self.lr * self._gradient(xi, yi)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "        pred = sigmoid(X @ self.w)\n",
    "        return pred > self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.3 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1 epochs, the accuracy reaches 85.80246913580247% using lr=0.01\n",
      "In 10 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 100 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 1000 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [1,10,100,1000] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.1, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.05, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.01, the accuracy reaches 98.76543209876543% in 100 epochs\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [ 0.5, 0.1, 0.05, 0.01] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStochastic gradient descent\\nhas negligible total CPU time but a higher wall time compared to batch gradient descent\\nit might be more efficient in terms of CPU usage, \\nit takes longer to complete due to the nature of updating parameters for each data point.\\n'"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Stochastic gradient descent\n",
    "\n",
    "CPU times: total: 0 ns\n",
    "Wall time: 3.99 ms\n",
    "\n",
    "it has a slightly higher wall time, \n",
    "indicating that it takes a bit longer in real-world time compared to batch gradient descent.\n",
    "\n",
    "it may require more iterations to converge due to the noisy updates.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Termination conditions (e.g. The gradient check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any premade algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                            # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducability\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multipled by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "        \n",
    "        pred = sigmoid(X @ self.w)\n",
    "        error = y - pred\n",
    "        gradient = -X.T @ error\n",
    "        return gradient\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "\n",
    "        # w(t+1) = w(t) - learning rate - gradient\n",
    "        self.w = self.w - self.lr * self._gradient(X,y)\n",
    "\n",
    "    def fit(self, X, y, threshold=1e-5, max_iterations=100):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "\n",
    "        prev_loss = float('inf')\n",
    "        itrations_no_change = 0\n",
    "\n",
    "        for epoch in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)    \n",
    "        \n",
    "            gradient = self._gradient(X, y)\n",
    "            curr_loss = np.linalg.norm(gradient)\n",
    "            \n",
    "            if np.abs(prev_loss - curr_loss) < threshold: # loss change less than a specific threshold\n",
    "                itrations_no_change += 1\n",
    "                if itrations_no_change == max_iterations:\n",
    "                    break\n",
    "            else:\n",
    "                itrations_no_change = 0\n",
    "                prev_loss = curr_loss\n",
    "                    \n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "        pred = sigmoid(X @ self.w)\n",
    "        return pred > self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.3 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 10 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 100 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n",
      "In 1000 epochs, the accuracy reaches 98.76543209876543% using lr=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "lr = 0.01 #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [1,10,100,1000] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.1, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.05, the accuracy reaches 98.76543209876543% in 100 epochs\n",
      "Using lr=0.01, the accuracy reaches 98.76543209876543% in 100 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "epochs = 100 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [ 0.5, 0.1, 0.05, 0.01] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 6.98 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.01\n",
    "epochs = 100\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPTOP\\AppData\\Local\\Temp\\ipykernel_8524\\692376189.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return  1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTermination conditions (e.g. The gradient check)\\nhas a negligible total CPU time \\nbut a significant wall time. \\nThis suggests that checking the termination condition adds some overhead to the process,\\ntaking up additional real-world time.\\n'"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Termination conditions (e.g. The gradient check)\n",
    "\n",
    "CPU times: total: 0 ns\n",
    "Wall time: 6.98 ms\n",
    "\n",
    "it has higher wall time, \n",
    "indicating that checking the termination condition takes a bit longer in real-world time.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd780a10ad03a506e232ec29f104692e8d999a77309c0fc915217df500c72051"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
